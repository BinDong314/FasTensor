

Overview:
1) run "tdms2h5" to convert TDMS files into HDF5 files
2) run "h5-vds-merge" to group all HDF5 files into a single HDF5
3) run "das-correlation" on the single HDF5 to calculate local similarity 

   * All tests are performed on snowbear.lbl.gov


1. Preparing steps:
  1.1. ArrayUDF
    > module purge
    > module load gcc/4.8.5 openmpi/3.1.2-gcc hdf5/1.11.3-gcc-p arrayudf/1.0.1-gcc  fftw/3.3.4-gcc git
    > module unload openmpi/1.10.2-gcc
    > git clone https://bitbucket.org/dbin_sdm/arrayudf-test.git
    > cd arrayudf-test/examples/das/   
    > make clean 
    > make
    > ls
      * Make sure that "tdms2h5", "das-correlation" and "h5-vds-merge" are generated successfully 


2. Usage

* Assume "~/testTDMSdata" directory contains all  TDMS files
* Merged file "westSac_merged.h5p" is under your home directory ~/

Step1: convert TDMS files (all in a directory ) to HDF5 files (in another directory)
   > ls   ~/testTDMSdata/
          westSac_170728224410.tdms westSac_170728224610.tdms
          westSac_170728224510.tdms westSac_170728224710.tdms
   > mkdir ~/testH5data
  
   > mpirun -n 2 tdms2h5 -i ~/testTDMSdata/ -o ~/testH5data -b
      * Depends on the files under testTDMSdata, you can 
      * change "-n 2" to have different parallel
      * Please refer to Section 4 for metadata
     
      * Data is stored as a 2D dataset namely "/DataTimeChannel"
      * in output HDF5 file

Step2: merge HDF5 files into a single HDF5 file
   > ls ~/testH5data/
        westSac_170728224410.tdms.h5  westSac_170728224610.tdms.h5
        westSac_170728224510.tdms.h5  westSac_170728224710.tdms.h5
  
   > cd arrayudf-test/examples/das/
  
   > ./h5-vds-merge -i ~/testH5data/  -o ~/westSac_merged.h5
  
   >  h5dump  -A ~/westSac_merged.h5
      GROUP "/" {
      DATASET "DataTimeChannel" {
          DATATYPE  H5T_IEEE_F32LE
          DATASPACE  SIMPLE { ( 120000, 11648 ) / ( 120000, 11648 ) }
      }}}


Step3: ArrayUDF Code: das-correlation
   > cd arrayudf-test/examples/das/
  
   > rm  ~/westSac_merged_arrayudf.h5p
     *  Please remove the output file first
  
   > mpirun -n 8  ./das-correlation -i ~/westSac_merged.h5 -o ~/westSac_merged_arrayudf.h5 -g / -d /DataTimeChannel -n 2 -c 120000,32 -t 0,20 -s 1000,1 -e 500 -w 300 -l 10
     *  See Section 3 for meaning of parameters 

   > h5dump  -A  ~/westSac_merged_arrayudf.h5
      GROUP "/" {
      DATASET "DataTimeChannel" {
          DATATYPE  H5T_IEEE_F32LE
          DATASPACE  SIMPLE { ( 120, 11648 ) / ( 120, 11648 ) }
      }}}


    > Simple test with the result with "plot.py"
    > module load python/3.5.1
    > python3 plot.py
      * you may need to edit plot.py's "filename" to point right file
      * Also, you may need "ssh -Y username@snowbear.lbl.gov" to see the figure 

    So far, you make it and the "westSac_merged_arrayudf.h5p" is the result file

  Step4: ArrayUDF Code: das-fft
    das-fft can be compiled with the same commands in Step3.

    > mpirun  -n 1 ./das-fft -i ./test-data/fft-test.h5 -o ./test-data/fft-test.arrayudf.h5  -g / -t /white -x /Xcorr
    > h5dump  -d /Xcorr -s 0,0 -c 4,4 ./test-data/fft-test.arrayudf.h5
      HDF5 "./test-data/fft-test.arrayudf.h5" {
      DATASET "/Xcorr" {
        DATATYPE  H5T_IEEE_F32LE
        DATASPACE  SIMPLE { ( 14999, 101 ) / ( 14999, 101 ) }
        SUBSET {
            START ( 0, 0 );
            STRIDE ( 1, 1 );
            COUNT ( 4, 4 );
            BLOCK ( 1, 1 );
            DATA {
            (0,0): 4.71574e-05, 8.80479e-05, 1.17999e-06, -4.70807e-06,
            (1,0): 8.27977e-05, 0.000147256, 1.76964e-05, -4.86011e-06,
            (2,0): 6.25906e-05, 9.84364e-05, 8.96957e-06, -4.6792e-06,
            (3,0): 5.10183e-05, 3.64967e-05, -3.38699e-06, 2.81772e-05
            }
        }
      }
      }


3. Parameters for das-correlation

  -i input HDF5 file
  -o output HDF5 file
  -g group within HDF5
  -d dataset within HDF5
  -n the dimension of input dataset, it is always 2 for das data
  -c chunk size which breaks the input data into subsets for parallel processing

  -t overlap among chunks (to be aligned with -l parameters)
  -s skip points, e.g., 1000,1 computes local similarity every 1000 points on first dimension 
  -e size of time window along single channel 
  -w counts of windows along single channel
  -l channel offset ()


4. Parameters for das-fft

  -h help (--help)
  -i input file
  -o output file
	-g group name (path) for both input and output
  -t dataset name for input time series
  -x dataset name for output correlation
  -w window size (only used when window size is different from chunk_size[0])
  -m index of master channel (0 by default )
  Example: mpirun -n 1 ./das-fft -i ./test-data/fft-test.h5 -o ./test-data/fft-test.arrayudf.h5  -g / -t /white -x /Xcorr

5. Metadata 

  Overall, "tdms2h5" tries to extract all metadata from the header of TDMS file and store 
  them as attributes (or groups) in HDF5. In a TDMS file,  it has 28 byte fixed 
  LEADIN header and user-customized metadata. The user-customized metadata has over two 
  levels, object and property (of object). Corresponding, we use HDF5 to represent object 
  in TDMS and attach property as HDF5 attribute for group. By default, the root group '/'
  in HDF5 is the "/" object in TDMS file.  "/Measurement" is the object (group) under the 
  root '/' object (group). "/Measurement/1" is another object (group) under "/Measurement".


  In case of missing something in our code, we also store the whole raw metadata section as 
  a single dataset "/RawMetadataTDMS" with "H5T_OPAQUE". So, users can explain the TDMS 
  metadata by themselves. 

  Example of metadata can be found: 

  > h5dump  -A test.tdms.h5
  
  HDF5 "test.tdms.h5" {
  GROUP "/" {
    ATTRIBUTE "Acoustic Output" {
        DATATYPE  H5T_STRING {
          STRSIZE 12;
          STRPAD H5T_STR_NULLTERM;
          CSET H5T_CSET_ASCII;
          CTYPE H5T_C_S1;
        }
        DATASPACE  SCALAR
        DATA {
        (0): "Differential"
        }
    }
    ATTRIBUTE "Attenuator Voltage (V)" {
        DATATYPE  H5T_IEEE_F64BE
        DATASPACE  SIMPLE { ( 1 ) / ( 1 ) }
        DATA {
        (0): 2.29042
        }
    }

      ....
      DATASET "DataTimeChannel" {
        DATATYPE  H5T_STD_I16BE
        DATASPACE  SIMPLE { ( 30000, 11648 ) / ( 30000, 11648 ) }
    }
    DATASET "RawMetadataTDMS" {
        DATATYPE  H5T_OPAQUE {
          OPAQUE_TAG "RAW Metadata from TDMS file as OPAQUE type";
        }
        DATASPACE  SIMPLE { ( 577536 ) / ( 577536 ) }
    }





6. Compile ArrayUDF on Snowbear

  module purge
  module load gcc/4.8.5           
  module load openmpi/3.1.2-gcc   
  module load hdf5/1.11.3-gcc-p  
  module load fftw/3.3.4-gcc
  module unload openmpi/1.10.2-gcc
  module load automake

 > ./autogen.sh
 >  CXX=mpiCC FC=mpif90 ./configure --prefix=$PWD/build --with-hdf5=/usr/local/software/sl-6.x86_64/modules/hdf5/1.11.3-gcc-p --enable-shared
 > make install
 > cd example/das
 > edit Makefile to have correct HDF5/MPICC etc.
 > make 




