

================= =======
Old usage.example 
================= =======




Something to know before:
 * The h5py and ArrayUDF uses two different versions of HDF5
 *  The major reason is that h5py can not work on latest HDF5
 *  But, the ArrayUDf needs latest HDF5 for parallel virtual dataset reading.


Preparing steps (on snowbear only):

  1) nptdms  
    > module purge
    > module load  python/3.5.1
    > pip3 install nptdms
      * Find more info about nptdms at : https://github.com/adamreeve/npTDMS

  2) h5py 
    >  module purge
    >  module load  gcc/4.8.5 hdf5/1.10.1-gcc-s python/3.5.1 
    >  wget https://github.com/h5py/h5py/archive/master.zip
         * Note: "git clone h5py" does not work on snowbear
    >  pip3 install cython --user
    >  python3 setup.py configure  --reset
         * you may need this step if --reset find wrong HDF5
         * python3 setup.py configure  --hdf5=/usr/local/software/sl-6.x86_64/modules/hdf5/1.10.1-gcc-s
         * For paralle h5py you need use --mpi. We are here fine with sequantial one
    >  python3 setup.py build
    >  python3 setup.py install --user


Three steps to start with sample data. More advanced features can be obtained by "-h"
  * Assume "~/testTDMSdata" directory contains all your *tdms files
  * Merged file "westSac_merged.h5p" is under your home directory ~/

Step1: convert TDMS files (all in a directory ) to HDF5 files (in another directory)

   > ls   ~/testTDMSdata/
          westSac_170728224410.tdms	westSac_170728224610.tdms
	        westSac_170728224510.tdms	westSac_170728224710.tdms
   > mkdir testH5data
   > python3 tdms2h5.py -i ~/testTDMSdata/ -o ~/testH5data -b
       * it takes a while
    
Step2: merge HDF5 files into a single HDF5 file

   > ls ~/testH5data/
      westSac_170728224410.h5p	westSac_170728224610.h5p
      westSac_170728224510.h5p	westSac_170728224710.h5p
   > python h5-merge.py -i ~/testH5data/ -o ~/westSac_merged.h5p
   > h5dump -A westSac_merged.h5p
     HDF5 "westSac_merged.h5p" {
     	  GROUP "/" {
	     DATASET "DataByChannelTime" {
	           DATATYPE  H5T_STD_I16LE
		         DATASPACE  SIMPLE { ( 120000, 11648 ) / ( 120000, 11648 ) }
     }}}


Step3: ArrayUDF Code
  > module purge
  > module load gcc/4.8.5 openmpi/3.1.2-gcc hdf5/1.11.3-gcc-p arrayudf/1.0.0-gcc git
     *Note here: you need a different version of HDF5 
  > git clone https://bitbucket.org/dbin_sdm/arrayudf-test.git
  > cd arrayudf-test/examples/das/   
  > make clean 
  > make
  > mpirun -n 4  ./das-correlation -i ~/westSac_merged.h5p -o ~/westSac_merged_arrayudf.h5p -g / -d /DataByChannelTime -n 2 -c 30000,
32 -t 0,20 -s 1000,1 -e 500 -w 300 -l 10
  > h5dump -A ~/westSac_merged_arrayudf.h5p 
    GROUP "/" {
     DATASET "DataByChannelTime" {
      DATATYPE  H5T_IEEE_F32LE
      DATASPACE  SIMPLE { ( 120, 11648 ) / ( 120, 11648 ) }
   }}}


Others:

How to install New HDF5 for ArrayUDF by yourself:

        git clone https://bitbucket.hdfgroup.org/scm/hdffv/hdf5.git
        cd hdf5
        git checkout parallel_vds_develop

        module load gcc/4.8.5
        module load openmpi/3.1.2-gcc
        module load libtool/2.4.6
        module load automake/1.15

        ./autogen.sh
        ./configure --enable-parallel --prefix=/bearbin-data3/home/dbin/soft/hdf5/build 
        make
        make install
        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/soft/hdf5/build/lib/





Others:

How to install New HDF5 for ArrayUDF by yourself:

        git clone https://bitbucket.hdfgroup.org/scm/hdffv/hdf5.git
        cd hdf5
        git checkout parallel_vds_develop

        module load gcc/4.8.5
        module load openmpi/3.1.2-gcc
        module load libtool/2.4.6
        module load automake/1.15

        ./autogen.sh
        ./configure --enable-parallel --prefix=/bearbin-data3/home/dbin/soft/hdf5/build 
        make
        make install
        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/soft/hdf5/build/lib/








